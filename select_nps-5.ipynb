{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am trying to change the codes (4) in my downloads, to improve \n",
    "    1- the hard-coded directory names to a more relative path with the use of os.getcwd() (from the website 'tutorialspoint.com'). Otherwise, the codes cannot be functional on any other device, and not mine too!\n",
    "    2- the problem with dumping files which seems to be based on an indentation imprecision. For this I just played around with indentations and lines of codes. \n",
    "\n",
    "but then, apparently I do not have the right input file. So, let's discuss this in the course! :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Get the current directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Define the directory containing the input files\n",
    "input_directory = os.path.join(current_directory, \"parsed_data\")\n",
    "\n",
    "# Go through the files in the input directory\n",
    "for filename in glob.glob(os.path.join(input_directory, '*.txt')):\n",
    "    # List to store ALL nps of ONE FILE in\n",
    "    nps_of_file = list()\n",
    "\n",
    "    sentence_counter = 1\n",
    "    np_counter = 1\n",
    "\n",
    "     # Open the file in read-mode\n",
    "    with open(filename, 'r', encoding=\"UTF-8-sig\") as file:\n",
    "        # Read in the lines of token\n",
    "        file_input = file.readlines()\n",
    "\n",
    "        # List to generate a single sentence out of the file\n",
    "        single_sentence = list()\n",
    "\n",
    "        # Go through each line/\"sentence\" of the file\n",
    "        for line in file_input:\n",
    "            # If it is not an empty line/\"sentence\", add it to the list of a single sentence\n",
    "            if line[0] != \"\\n\":\n",
    "                single_sentence.append(line)\n",
    "\n",
    "            # Empty line/\"sentence\" indicates the end of a sentence\n",
    "            elif line[0] == \"\\n\":\n",
    "                # Split each element of the sentence into the single token-information\n",
    "                sentence_as_elements = list()\n",
    "                for row in single_sentence:\n",
    "                    elements = row.split(\"\\t\")\n",
    "                    sentence_as_elements.append(elements)\n",
    "\n",
    "                # Create DataFrame with column names\n",
    "                df = pd.DataFrame(sentence_as_elements, columns=[\"ID\", \"token\", \"lemma\", \"wordtype\", \"POSTag\", \"features\", \"head\", \"deprel\", \"dunno1\", \"dunno2\"])\n",
    "\n",
    "                # Delete columns we do not need\n",
    "                df = df.drop([\"dunno1\", \"dunno2\"], axis=1)\n",
    "\n",
    "                # Extract the whole token-column to get the whole sentence as tokens\n",
    "                sentence_list = df[\"token\"].values.tolist()\n",
    "                sentence_as_string = \" \".join(sentence_list)\n",
    "                number_of_tokens_of_sentence = len(sentence_list)\n",
    "\n",
    "                token_head_list = list()\n",
    "                token_index_list = list()\n",
    "                token_id_list = list()\n",
    "\n",
    "                # Generate dictionary\n",
    "                # Go through the DataFrame of the file\n",
    "                for index, row in df.iterrows():\n",
    "                    token_head_list.append([[row[\"token\"]], row[\"head\"]])\n",
    "\n",
    "                    # Find the root\n",
    "                    if row[\"deprel\"] == \"root\":\n",
    "                        if str(row[\"deprel\"]).isalpha():\n",
    "                            root_of_sentence = df.loc[index, :].values.flatten().tolist()\n",
    "                        else:\n",
    "                            if root_of_sentence:\n",
    "                                root_of_sentence = \"None\"\n",
    "                            else:\n",
    "                                pass\n",
    "\n",
    "                    # Add the token and its index to the other dictionary\n",
    "                    token_index_list.append([[row[\"token\"]], index])\n",
    "                    token_id_list.append([[row[\"token\"]], row[\"ID\"]])\n",
    "\n",
    "                # List for all NPs in the sentence\n",
    "                all_nps_of_sentence = list()\n",
    "\n",
    "                # Go through the DataFrame once again\n",
    "                for index, row in df.iterrows():\n",
    "                    # If the wordtype is a noun\n",
    "                    if row[\"wordtype\"] == \"N\":\n",
    "                        # Create list for the NP:\n",
    "                        current_np_ids = list()\n",
    "                        # Get all the info of the noun\n",
    "                        info_of_noun = df.loc[index, :].values.flatten().tolist()\n",
    "                        id_of_noun = info_of_noun[0]\n",
    "                        head_of_noun_ID = info_of_noun[6]\n",
    "                        # Append the noun's ID to the list\n",
    "                        current_np_ids.append(id_of_noun)\n",
    "                        if int(head_of_noun_ID) != 0:\n",
    "                            current_np_ids.append(head_of_noun_ID)\n",
    "\n",
    "                        # In the following:\n",
    "                        # Check if any word refers to the noun as head\n",
    "                        for ind, token_head_pair in enumerate(token_head_list):\n",
    "                            # Check if the ID of the noun is the deprel of another token --> noun is head of that token\n",
    "                            if id_of_noun == token_head_pair[1]:\n",
    "                                # Append the ID of the token of the sentence\n",
    "                                # --> Get a list with IDs, can sort them and then create a second list with the filled-in token (information)\n",
    "                                current_np_ids.append(token_id_list[ind][1])\n",
    "\n",
    "                        current_np_ids.sort()\n",
    "                        number_of_tokens_of_np = len(current_np_ids)\n",
    "\n",
    "                        # Create NP with the regarding token information, based on the IDs\n",
    "                        current_np = list()\n",
    "                        for token_id in current_np_ids:\n",
    "                            index_in_df = int(token_id) - 1\n",
    "                            if int(index_in_df) < 0:\n",
    "                                token_info = tuple(df.loc[int(token_id), :].values.flatten().tolist())\n",
    "                            else:\n",
    "                                token_info = tuple(df.loc[int(index_in_df), :].values.flatten().tolist())\n",
    "                            current_np.append(token_info)\n",
    "\n",
    "                        \"\"\"\n",
    "                        Create the final form of the single np on form of:\n",
    "                        [   Satz_NP-Nummer,\n",
    "                            Dateiname {str},\n",
    "                            Satz_ID_NR {int},\n",
    "                            Satz {str},\n",
    "                            (root, POS-Tag, grammatical_info) {tuple},\n",
    "                            Anzahl-der-Token-der-NP {int},\n",
    "                            [NP] {list}, Bsp.: [(token1, POS-tag, grammatical_info),... (token-n, POS-tag, grammatical_info)],\n",
    "                        \"\"\"\n",
    "                        sentence_np_id = str(sentence_counter) + \"_\" + str(np_counter)\n",
    "                        final_form_of_np = [sentence_np_id, filename, sentence_counter, sentence_as_string, root_of_sentence, number_of_tokens_of_np, current_np]\n",
    "                        np_counter += 1\n",
    "                        all_nps_of_sentence.append(final_form_of_np)\n",
    "\n",
    "                # Clear the list of the single sentence for the new sentence\n",
    "                single_sentence = list()\n",
    "                sentence_counter += 1\n",
    "                nps_of_file.extend(all_nps_of_sentence)\n",
    "\n",
    "    filename_2_0 = os.path.splitext(filename)[0]\n",
    "    newfilename = filename_2_0 + \"_exported_nps.txt\"\n",
    "\n",
    "    # Print each individual element of nps_of_file to the new txt-file\n",
    "    new_filepath = os.path.join(newfilename)\n",
    "    with open(new_filepath, \"a\", encoding=\"UTF-8-sig\") as f:\n",
    "        for nps in nps_of_file:\n",
    "            for item in nps:\n",
    "                print(item, file=f)\n",
    "            print(file=f)\n",
    "\n",
    "    # Save as a pickle file\n",
    "    pickle_filename = filename_2_0 + \"_exported_nps.pkl\"\n",
    "    pickle_filepath = os.path.join(pickle_filename)\n",
    "    with open(pickle_filepath, 'wb') as f:\n",
    "        pickle.dump(nps_of_file, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
