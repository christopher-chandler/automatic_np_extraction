{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am trying to change the codes (4) in my downloads, to improve \n",
    "    1- the hard-coded directory names to a more relative path with the use of os.getcwd() (from the website 'tutorialspoint.com'). Otherwise, the codes cannot be functional on any other device, and not mine too!\n",
    "    2- the problem with dumping files which seems to be based on an indentation imprecision. For this I just played around with indentations and lines of codes. \n",
    "\n",
    "but then, apparently I do not have the right input file. So, let's discuss this in the course! :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "10 columns passed, passed data had 1 columns",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\construction.py:969\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[1;34m(content, columns, dtype)\u001b[0m\n\u001b[0;32m    968\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 969\u001b[0m     columns \u001b[39m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[0;32m    970\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    971\u001b[0m     \u001b[39m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\construction.py:1017\u001b[0m, in \u001b[0;36m_validate_or_indexify_columns\u001b[1;34m(content, columns)\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_mi_list \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(columns) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(content):  \u001b[39m# pragma: no cover\u001b[39;00m\n\u001b[0;32m   1016\u001b[0m     \u001b[39m# caller's responsibility to check for this...\u001b[39;00m\n\u001b[1;32m-> 1017\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[0;32m   1018\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(columns)\u001b[39m}\u001b[39;00m\u001b[39m columns passed, passed data had \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1019\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(content)\u001b[39m}\u001b[39;00m\u001b[39m columns\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1020\u001b[0m     )\n\u001b[0;32m   1021\u001b[0m \u001b[39melif\u001b[39;00m is_mi_list:\n\u001b[0;32m   1022\u001b[0m \n\u001b[0;32m   1023\u001b[0m     \u001b[39m# check if nested list column, length of each sub-list should be equal\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: 10 columns passed, passed data had 1 columns",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 44\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[39m# Create DataFrame with column names\u001b[39;00m\n\u001b[0;32m     43\u001b[0m column_names \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mID\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtoken\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlemma\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mwordtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mPOSTag\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfeatures\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mhead\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mdeprel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mdunno1\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mdunno2\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m---> 44\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame(sentence_as_elements, columns\u001b[39m=\u001b[39;49mcolumn_names)\n\u001b[0;32m     46\u001b[0m \u001b[39m# Delete columns we do not need\u001b[39;00m\n\u001b[0;32m     47\u001b[0m df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mdrop([\u001b[39m\"\u001b[39m\u001b[39mdunno1\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mdunno2\u001b[39m\u001b[39m\"\u001b[39m], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:746\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    744\u001b[0m     \u001b[39mif\u001b[39;00m columns \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    745\u001b[0m         columns \u001b[39m=\u001b[39m ensure_index(columns)\n\u001b[1;32m--> 746\u001b[0m     arrays, columns, index \u001b[39m=\u001b[39m nested_data_to_arrays(\n\u001b[0;32m    747\u001b[0m         \u001b[39m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;49;00m\n\u001b[0;32m    748\u001b[0m         \u001b[39m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;49;00m\n\u001b[0;32m    749\u001b[0m         data,\n\u001b[0;32m    750\u001b[0m         columns,\n\u001b[0;32m    751\u001b[0m         index,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    752\u001b[0m         dtype,\n\u001b[0;32m    753\u001b[0m     )\n\u001b[0;32m    754\u001b[0m     mgr \u001b[39m=\u001b[39m arrays_to_mgr(\n\u001b[0;32m    755\u001b[0m         arrays,\n\u001b[0;32m    756\u001b[0m         columns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    759\u001b[0m         typ\u001b[39m=\u001b[39mmanager,\n\u001b[0;32m    760\u001b[0m     )\n\u001b[0;32m    761\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\construction.py:510\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[1;34m(data, columns, index, dtype)\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[39mif\u001b[39;00m is_named_tuple(data[\u001b[39m0\u001b[39m]) \u001b[39mand\u001b[39;00m columns \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    508\u001b[0m     columns \u001b[39m=\u001b[39m ensure_index(data[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39m_fields)\n\u001b[1;32m--> 510\u001b[0m arrays, columns \u001b[39m=\u001b[39m to_arrays(data, columns, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m    511\u001b[0m columns \u001b[39m=\u001b[39m ensure_index(columns)\n\u001b[0;32m    513\u001b[0m \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\construction.py:875\u001b[0m, in \u001b[0;36mto_arrays\u001b[1;34m(data, columns, dtype)\u001b[0m\n\u001b[0;32m    872\u001b[0m     data \u001b[39m=\u001b[39m [\u001b[39mtuple\u001b[39m(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m data]\n\u001b[0;32m    873\u001b[0m     arr \u001b[39m=\u001b[39m _list_to_arrays(data)\n\u001b[1;32m--> 875\u001b[0m content, columns \u001b[39m=\u001b[39m _finalize_columns_and_data(arr, columns, dtype)\n\u001b[0;32m    876\u001b[0m \u001b[39mreturn\u001b[39;00m content, columns\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\construction.py:972\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[1;34m(content, columns, dtype)\u001b[0m\n\u001b[0;32m    969\u001b[0m     columns \u001b[39m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[0;32m    970\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    971\u001b[0m     \u001b[39m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n\u001b[1;32m--> 972\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(err) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m    974\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(contents) \u001b[39mand\u001b[39;00m contents[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mobject_:\n\u001b[0;32m    975\u001b[0m     contents \u001b[39m=\u001b[39m _convert_object_array(contents, dtype\u001b[39m=\u001b[39mdtype)\n",
      "\u001b[1;31mValueError\u001b[0m: 10 columns passed, passed data had 1 columns"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Get the current directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Define the directory containing the input files\n",
    "input_directory = os.path.join(current_directory, \"parsed_data\")\n",
    "\n",
    "# Go through the files in the input directory\n",
    "for filename in glob.glob(os.path.join(input_directory, '*.txt')):\n",
    "    # List to store ALL nps of ONE FILE in\n",
    "    nps_of_file = list()\n",
    "\n",
    "    sentence_counter = 1\n",
    "    np_counter = 1\n",
    "\n",
    "     # Open the file in read-mode\n",
    "    with open(filename, 'r', encoding=\"UTF-8-sig\") as file:\n",
    "        # Read in the lines of token\n",
    "        file_input = file.readlines()\n",
    "\n",
    "        # List to generate a single sentence out of the file\n",
    "        single_sentence = list()\n",
    "\n",
    "        # Go through each line/\"sentence\" of the file\n",
    "        for line in file_input:\n",
    "            # If it is not an empty line/\"sentence\", add it to the list of a single sentence\n",
    "            if line[0] != \"\\n\":\n",
    "                single_sentence.append(line)\n",
    "\n",
    "            # Empty line/\"sentence\" indicates the end of a sentence\n",
    "            elif line[0] == \"\\n\":\n",
    "                # Split each element of the sentence into the single token-information\n",
    "                sentence_as_elements = list()\n",
    "                for row in single_sentence:\n",
    "                    elements = row.split(\"\\t\")\n",
    "                    sentence_as_elements.append(elements)\n",
    "\n",
    "                # Create DataFrame with column names\n",
    "                df = pd.DataFrame(sentence_as_elements, columns=[\"ID\", \"token\", \"lemma\", \"wordtype\", \"POSTag\", \"features\", \"head\", \"deprel\", \"dunno1\", \"dunno2\"])\n",
    "\n",
    "                # Delete columns we do not need\n",
    "                df = df.drop([\"dunno1\", \"dunno2\"], axis=1)\n",
    "\n",
    "                # Extract the whole token-column to get the whole sentence as tokens\n",
    "                sentence_list = df[\"token\"].values.tolist()\n",
    "                sentence_as_string = \" \".join(sentence_list)\n",
    "                number_of_tokens_of_sentence = len(sentence_list)\n",
    "\n",
    "                token_head_list = list()\n",
    "                token_index_list = list()\n",
    "                token_id_list = list()\n",
    "\n",
    "                # Generate dictionary\n",
    "                # Go through the DataFrame of the file\n",
    "                for index, row in df.iterrows():\n",
    "                    token_head_list.append([[row[\"token\"]], row[\"head\"]])\n",
    "\n",
    "                    # Find the root\n",
    "                    if row[\"deprel\"] == \"root\":\n",
    "                        if str(row[\"deprel\"]).isalpha():\n",
    "                            root_of_sentence = df.loc[index, :].values.flatten().tolist()\n",
    "                        else:\n",
    "                            if root_of_sentence:\n",
    "                                root_of_sentence = \"None\"\n",
    "                            else:\n",
    "                                pass\n",
    "\n",
    "                    # Add the token and its index to the other dictionary\n",
    "                    token_index_list.append([[row[\"token\"]], index])\n",
    "                    token_id_list.append([[row[\"token\"]], row[\"ID\"]])\n",
    "\n",
    "                # List for all NPs in the sentence\n",
    "                all_nps_of_sentence = list()\n",
    "\n",
    "                # Go through the DataFrame once again\n",
    "                for index, row in df.iterrows():\n",
    "                    # If the wordtype is a noun\n",
    "                    if row[\"wordtype\"] == \"N\":\n",
    "                        # Create list for the NP:\n",
    "                        current_np_ids = list()\n",
    "                        # Get all the info of the noun\n",
    "                        info_of_noun = df.loc[index, :].values.flatten().tolist()\n",
    "                        id_of_noun = info_of_noun[0]\n",
    "                        head_of_noun_ID = info_of_noun[6]\n",
    "                        # Append the noun's ID to the list\n",
    "                        current_np_ids.append(id_of_noun)\n",
    "                        if int(head_of_noun_ID) != 0:\n",
    "                            current_np_ids.append(head_of_noun_ID)\n",
    "\n",
    "                        # In the following:\n",
    "                        # Check if any word refers to the noun as head\n",
    "                        for ind, token_head_pair in enumerate(token_head_list):\n",
    "                            # Check if the ID of the noun is the deprel of another token --> noun is head of that token\n",
    "                            if id_of_noun == token_head_pair[1]:\n",
    "                                # Append the ID of the token of the sentence\n",
    "                                # --> Get a list with IDs, can sort them and then create a second list with the filled-in token (information)\n",
    "                                current_np_ids.append(token_id_list[ind][1])\n",
    "\n",
    "                        current_np_ids.sort()\n",
    "                        number_of_tokens_of_np = len(current_np_ids)\n",
    "\n",
    "                        # Create NP with the regarding token information, based on the IDs\n",
    "                        current_np = list()\n",
    "                        for token_id in current_np_ids:\n",
    "                            index_in_df = int(token_id) - 1\n",
    "                            if int(index_in_df) < 0:\n",
    "                                token_info = tuple(df.loc[int(token_id), :].values.flatten().tolist())\n",
    "                            else:\n",
    "                                token_info = tuple(df.loc[int(index_in_df), :].values.flatten().tolist())\n",
    "                            current_np.append(token_info)\n",
    "\n",
    "                        \"\"\"\n",
    "                        Create the final form of the single np on form of:\n",
    "                        [   Satz_NP-Nummer,\n",
    "                            Dateiname {str},\n",
    "                            Satz_ID_NR {int},\n",
    "                            Satz {str},\n",
    "                            (root, POS-Tag, grammatical_info) {tuple},\n",
    "                            Anzahl-der-Token-der-NP {int},\n",
    "                            [NP] {list}, Bsp.: [(token1, POS-tag, grammatical_info),... (token-n, POS-tag, grammatical_info)],\n",
    "                        \"\"\"\n",
    "                        sentence_np_id = str(sentence_counter) + \"_\" + str(np_counter)\n",
    "                        final_form_of_np = [sentence_np_id, filename, sentence_counter, sentence_as_string, root_of_sentence, number_of_tokens_of_np, current_np]\n",
    "                        np_counter += 1\n",
    "                        all_nps_of_sentence.append(final_form_of_np)\n",
    "\n",
    "                # Clear the list of the single sentence for the new sentence\n",
    "                single_sentence = list()\n",
    "                sentence_counter += 1\n",
    "                nps_of_file.extend(all_nps_of_sentence)\n",
    "\n",
    "    filename_2_0 = os.path.splitext(filename)[0]\n",
    "    newfilename = filename_2_0 + \"_exported_nps.txt\"\n",
    "\n",
    "    # Print each individual element of nps_of_file to the new txt-file\n",
    "    new_filepath = os.path.join(folder, newfilename)\n",
    "    with open(new_filepath, \"a\", encoding=\"UTF-8-sig\") as f:\n",
    "        for nps in nps_of_file:\n",
    "            for item in nps:\n",
    "                print(item, file=f)\n",
    "            print(file=f)\n",
    "\n",
    "    # Save as a pickle file\n",
    "    pickle_filename = filename_2_0 + \"_exported_nps.pkl\"\n",
    "    pickle_filepath = os.path.join(folder, pickle_filename)\n",
    "    with open(pickle_filepath, 'wb') as f:\n",
    "        pickle.dump(nps_of_file, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
